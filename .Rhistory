latitude <- c("31.9566")
radius <- "50km"
latlong <- paste(latitude,longitude,radius,sep=",")
latlong <- rep(latlong, length(search_terms))
search_terms <- as.data.frame(cbind(latlong, search_terms))
search_terms$search_terms <- as.character(search_terms$search_terms)
search_terms$latlong <- as.character(search_terms$latlong)
search_terms$location <- location
# Loop through the data frame to look for tweets
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
names(tweets)
df <- count(tweets, c("longitude","latitude"))
df <- na.omit(df)
longitude
latitude
xmap <- get_map(location=c(lon=longitude,lat=latitude), zoom=10, maptype="hybrid")
xmap <- get_map(location=cbind(lon=longitude,lat=latitude), zoom=10, maptype="hybrid")
xmap <- get_map(location=rbind(lon=longitude,lat=latitude), zoom=10, maptype="hybrid")
xmap <- get_map(location=c(lon=-117.979146,lat=33.959581), zoom=10, maptype="hybrid")
xmap
finalmap <- ggmap(xmap, base_layer = ggplot(aes(x=longitude, y=latitude), data = df))
str(df)
df$longitude <- as.numeric(df$longitude)
df$latitude <- as.numeric(df$latitude)
finalmap +
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df) +
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow") +
geom_point(color="grey80", size = (df$freq)/10)
xmap <- get_map(location=c(lon=35.9457,lat=31.9566), zoom=10, maptype="hybrid")
finalmap <- ggmap(xmap, base_layer = ggplot(aes(x=longitude, y=latitude), data = df))
finalmap
finalmap +
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df) +
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow") +
geom_point(color="grey80", size = (df$freq)/10)
library('plyr')
library('rgdal')
library('maptools')
library('ggmap')
library(devtools)
library(twitteR)
api_key <- "aAJGoyTFFYNsnOBL8Aty2tLUX"
api_secret <- "xMLvTMQVaYG8QDjZTYhCWUkhjozYocGKLjR74aVbcQlLdT2Taa"
access_token <- "2876138241-aXmpy17m00CHCdo0zscOrQwL539UsLY3zfchLup"
access_token_secret <- "O5n7orFiPQBECMASbTPCD886kLxa9SK8HbMK7wqSj4L9D"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
search_terms <- c("water", "tanker truck", "Miyahuna")
#Coordinates of Amman
#location <- "Frankfurt"
#longitude <- c("8.6821")
#latitude <- c("50.1109")
location <- "Amman"
longitude <- c("35.9457")
latitude <- c("31.9566")
#location <- "Los Angeles"
#longitude <- c("-117.979146")
#latitude <- c("33.959581")
radius <- "15km"
latlong <- paste(latitude,longitude,radius,sep=",")
latlong <- rep(latlong, length(search_terms))
search_terms <- as.data.frame(cbind(latlong, search_terms))
search_terms$search_terms <- as.character(search_terms$search_terms)
search_terms$latlong <- as.character(search_terms$latlong)
search_terms$location <- location
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
names(tweets)
df <- count(tweets, c("longitude","latitude"))
df <- na.omit(df)
df
tweets
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
df
names(tweets)
df <- count(tweets, c("longitude","latitude"))
df <- na.omit(df)
df
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
search_terms <- c("water", "tanker truck", "Miyahuna")
#Coordinates of Amman
#location <- "Frankfurt"
#longitude <- c("8.6821")
#latitude <- c("50.1109")
location <- "Amman"
longitude <- c("35.9457")
latitude <- c("31.9566")
#location <- "Los Angeles"
#longitude <- c("-117.979146")
#latitude <- c("33.959581")
radius <- "15km"
latlong <- paste(latitude,longitude,radius,sep=",")
latlong <- rep(latlong, length(search_terms))
search_terms <- as.data.frame(cbind(latlong, search_terms))
search_terms$search_terms <- as.character(search_terms$search_terms)
search_terms$latlong <- as.character(search_terms$latlong)
search_terms$location <- location
# Loop through the data frame to look for tweets
tweets <- data.frame()
for (i in 1:nrow(search_terms)){
print(paste("Looking for",search_terms$search_terms[i], "in", search_terms[i,]$location))
tw = searchTwitter(search_terms[i,]$search_terms,n=round(2000/nrow(search_terms)), geocode=search_terms[i,]$latlong, lang="en")
if (length(tw) == 0){
print(paste("No tweets found for", search_terms$search_terms[i], "in", location))
} else {
tweets=rbind(twListToDF(tw), tweets)
}
}
names(tweets)
df <- count(tweets, c("longitude","latitude"))
df
df <- na.omit(df)
df
tweets
df$longitude <- as.numeric(df$longitude)
df$latitude <- as.numeric(df$latitude)
xmap <- get_map(location=c(lon=35.9457,lat=31.9566), zoom=10, maptype="hybrid")
finalmap <- ggmap(xmap, base_layer = ggplot(aes(x=longitude, y=latitude), data = df))
finalmap +
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df) +
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow") +
geom_point(color="grey80", size = (df$freq)/10)
finalmap +
finalmap
finalmap <- ggmap(xmap, base_layer = ggplot(aes(x=longitude, y=latitude), data = df))
finalmap
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df)
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow")
geom_point(color="grey80", size = (df$freq)/10)
finalmap +
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df)
finalmap +
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow") +
geom_point(color="grey80", size = (df$freq)/10)+
stat_density2d(aes(x = longitude, y = latitude, fill = ..level.., alpha =level.., size=freq),
bins = 9, geom = "polygon",alpha=0.2,
data = df)
finalmap +
scale_fill_gradient(trans = "sqrt", low = "grey5", high = "greenyellow") +
geom_point(color="grey80", size = (df$freq)/10)
land <- function(tau.litter=2., tau.fast=20., tau.slow=500, eff.microbes=0.8,
plant.eq=500, NPP.eq=60., betaCO2=0.36, longevity=2, Q10=2.0,
n.limitation=0.2, disturb.factor=1){
# Argument list:
# tau.plants : turnover time for live plants (default = 2 yr)
# tau.litter : turnover time for dead plant material (default = 20 yr)
# tau.soil   : turnover time of (armored) soil organic matter (default = 1000 yr)
# eff.microbes : efficiency of microbial respiration
# fert.factor   : fractional increase in NPP per doubling of CO2
# Initialize a bunch of arrays for later plotting
nYears <- 500
NEE <- replicate(nYears,NA)
NPP <- replicate(nYears,NA)
resp.total <- replicate(nYears,NA)
resp.litter <- replicate(nYears,NA)
resp.fast <- replicate(nYears,NA)
resp.slow <- replicate(nYears,NA)
plant <- replicate(nYears,NA)
litter <- replicate(nYears,NA)
fast.soil <- replicate(nYears,NA)
slow.soil <- replicate(nYears,NA)
mortality <- replicate(nYears,NA)
# Initialize mass of carbon (kg C) in plants, soil, and passive pools
# These are calculated as steady-state solutions to the differential equations
eq.capacity <- plant.eq / (1-1/longevity)  # resource-limited "carrying capacity"
growth.rate  <- NPP.eq / (plant.eq*(1-plant.eq/eq.capacity))
death.rate <- growth.rate/longevity  # fractional death per year
# Initialize each pool (GtC) to be in equilibrium with NPP and decay
plant[1] <- plant.eq
litter[1] <- tau.litter * death.rate * plant[1]
fast.soil[1] <- tau.fast/tau.litter * (1-eff.microbes) * litter[1]
slow.soil[1] <- tau.slow/tau.fast * (1-eff.microbes) * fast.soil[1]
# Read and set up driver data for CO2, temperature, disturbance, & nutrients
# CO2 and temperature from a box model driven with IPCC SRES A2
driver.data <- read.table('data/driver.data.txt', header=TRUE)
CO2 <- driver.data$CO2
temp <- driver.data$temp
deforestation <- driver.data$deforestation * disturb.factor
abandonment <- driver.data$abandonment * disturb.factor
nutrient <- driver.data$nutrient * n.limitation
# Start plant capacity at equilibrium value
capacity <- eq.capacity
# Integrate the model
for (i in 1:(nYears-1)){
# Apply CO2 fertilization to growth (NPP)
enhanced.growth.rate <- growth.rate * (1.0 + betaCO2 * log(CO2[i]/CO2[1]))
# Subtract half of deforestation from plant capacity, but add back abandonment
capacity <- capacity + longevity * (-deforestation[i] + abandonment[i]*2)
# Enhance plant capacity due to nutrient deposition
eff.capacity <- capacity * (1 + nutrient[i])
# Apply nutrient limitation & fertilization to get updated NPP
NPP[i] <- enhanced.growth.rate * plant[i] * (1 - plant[i]/eff.capacity)
# Apply disturbance & mortality
mortality[i] <- death.rate * plant[i] + deforestation[i]
# Apply temperature enhancement of respiration
resp.enhancement <- Q10 ^ ((temp[i]-temp[1])/10)
# Calculate respiration for each pool
resp.litter[i] <- eff.microbes * litter[i]/tau.litter * resp.enhancement
resp.fast[i] <- eff.microbes * fast.soil[i]/tau.fast * resp.enhancement
resp.slow[i] <- slow.soil[i]/tau.slow * resp.enhancement
resp.total[i] <- resp.litter[i] + resp.fast[i] + resp.slow[i]
# Net ecosystem exchnage is repiration minus NPP
NEE[i] <- resp.total[i] - NPP[i]
# Update all the carbon pools in plants and soils
plant[i+1] <- plant[i] + NPP[i] - mortality[i]
litter[i+1] <- litter[i] + mortality[i] - litter[i]/tau.litter * resp.enhancement
fast.soil[i+1] <- fast.soil[i] + resp.enhancement * (
(1.-eff.microbes) * litter[i]/tau.litter -
fast.soil[i]/tau.fast)
slow.soil[i+1] <- slow.soil[i] + resp.enhancement * (
(1.-eff.microbes) * fast.soil[i]/tau.fast -
slow.soil[i]/tau.slow)
}
return(list(parameters=list(tau.litter=tau.litter, tau.fast=tau.fast, tau.slow=tau.slow,
eff.microbes=eff.microbes, plant.eq=plant.eq, NPP.eq=NPP.eq,
betaCO2=betaCO2, longevity=longevity, Q10=Q10,
n.limitation=n.limitation, disturb.factor=disturb.factor),
output=data.frame(NEE=NEE, plant=plant, litter=litter, fast.soil=fast.soil,
slow.soil=slow.soil, NPP=NPP, CO2=CO2, deforestation=deforestation,
resp.litter=resp.litter, resp.fast=resp.fast, resp.slow=resp.slow,
resp.total=resp.total, temp=temp, nutrient=nutrient, mortality=mortality)
))
}
land <- function(tau.litter=2., tau.fast=20., tau.slow=500, eff.microbes=0.8,
plant.eq=500, NPP.eq=60., betaCO2=0.36, longevity=2, Q10=2.0,
n.limitation=0.2, disturb.factor=1){
# Argument list:
# tau.plants : turnover time for live plants (default = 2 yr)
# tau.litter : turnover time for dead plant material (default = 20 yr)
# tau.soil   : turnover time of (armored) soil organic matter (default = 1000 yr)
# eff.microbes : efficiency of microbial respiration
# fert.factor   : fractional increase in NPP per doubling of CO2
# Initialize a bunch of arrays for later plotting
nYears <- 500
NEE <- replicate(nYears,NA)
NPP <- replicate(nYears,NA)
resp.total <- replicate(nYears,NA)
resp.litter <- replicate(nYears,NA)
resp.fast <- replicate(nYears,NA)
resp.slow <- replicate(nYears,NA)
plant <- replicate(nYears,NA)
litter <- replicate(nYears,NA)
fast.soil <- replicate(nYears,NA)
slow.soil <- replicate(nYears,NA)
mortality <- replicate(nYears,NA)
# Initialize mass of carbon (kg C) in plants, soil, and passive pools
# These are calculated as steady-state solutions to the differential equations
eq.capacity <- plant.eq / (1-1/longevity)  # resource-limited "carrying capacity"
growth.rate  <- NPP.eq / (plant.eq*(1-plant.eq/eq.capacity))
death.rate <- growth.rate/longevity  # fractional death per year
# Initialize each pool (GtC) to be in equilibrium with NPP and decay
plant[1] <- plant.eq
litter[1] <- tau.litter * death.rate * plant[1]
fast.soil[1] <- tau.fast/tau.litter * (1-eff.microbes) * litter[1]
slow.soil[1] <- tau.slow/tau.fast * (1-eff.microbes) * fast.soil[1]
# Read and set up driver data for CO2, temperature, disturbance, & nutrients
# CO2 and temperature from a box model driven with IPCC SRES A2
driver.data <- read.table('data/driver.data.txt', header=TRUE)
CO2 <- driver.data$CO2
temp <- driver.data$temp
deforestation <- driver.data$deforestation * disturb.factor
abandonment <- driver.data$abandonment * disturb.factor
nutrient <- driver.data$nutrient * n.limitation
# Start plant capacity at equilibrium value
capacity <- eq.capacity
# Integrate the model
for (i in 1:(nYears-1)){
# Apply CO2 fertilization to growth (NPP)
enhanced.growth.rate <- growth.rate * (1.0 + betaCO2 * log(CO2[i]/CO2[1]))
# Subtract half of deforestation from plant capacity, but add back abandonment
capacity <- capacity + longevity * (-deforestation[i] + abandonment[i]*2)
# Enhance plant capacity due to nutrient deposition
eff.capacity <- capacity * (1 + nutrient[i])
# Apply nutrient limitation & fertilization to get updated NPP
NPP[i] <- enhanced.growth.rate * plant[i] * (1 - plant[i]/eff.capacity)
# Apply disturbance & mortality
mortality[i] <- death.rate * plant[i] + deforestation[i]
# Apply temperature enhancement of respiration
resp.enhancement <- Q10 ^ ((temp[i]-temp[1])/10)
# Calculate respiration for each pool
resp.litter[i] <- eff.microbes * litter[i]/tau.litter * resp.enhancement
resp.fast[i] <- eff.microbes * fast.soil[i]/tau.fast * resp.enhancement
resp.slow[i] <- slow.soil[i]/tau.slow * resp.enhancement
resp.total[i] <- resp.litter[i] + resp.fast[i] + resp.slow[i]
# Net ecosystem exchnage is repiration minus NPP
NEE[i] <- resp.total[i] - NPP[i]
# Update all the carbon pools in plants and soils
plant[i+1] <- plant[i] + NPP[i] - mortality[i]
litter[i+1] <- litter[i] + mortality[i] - litter[i]/tau.litter * resp.enhancement
fast.soil[i+1] <- fast.soil[i] + resp.enhancement * (
(1.-eff.microbes) * litter[i]/tau.litter -
fast.soil[i]/tau.fast)
slow.soil[i+1] <- slow.soil[i] + resp.enhancement * (
(1.-eff.microbes) * fast.soil[i]/tau.fast -
slow.soil[i]/tau.slow)
}
return(list(parameters=list(tau.litter=tau.litter, tau.fast=tau.fast, tau.slow=tau.slow,
eff.microbes=eff.microbes, plant.eq=plant.eq, NPP.eq=NPP.eq,
betaCO2=betaCO2, longevity=longevity, Q10=Q10,
n.limitation=n.limitation, disturb.factor=disturb.factor),
output=data.frame(NEE=NEE, plant=plant, litter=litter, fast.soil=fast.soil,
slow.soil=slow.soil, NPP=NPP, CO2=CO2, deforestation=deforestation,
resp.litter=resp.litter, resp.fast=resp.fast, resp.slow=resp.slow,
resp.total=resp.total, temp=temp, nutrient=nutrient, mortality=mortality)
))
}
plot.land <- function(history){
attach(history)
years <- 1800:2299
orig.par <- par(no.readonly=TRUE)
par(mfrow=c(3,1), cex.lab=1.5, cex.axis=1.5, cex.main=2)
# Top panel is NEE
plot(years, NEE, main='Net Ecosystem Flux to Atmosphere', ylab='GtC/yr',
type='l', lwd=6)
grid(col='black')
# Second panel is Gross fluxes
plot.limits=range(c(NPP,resp.total),na.rm=TRUE)
plot(years, NPP, main='Gross Fluxes', ylab='GtC/yr', col='darkgreen',
type='l', lwd=6, ylim=plot.limits)
lines(years, resp.total, col='red', type='l', lwd=6)
legend('right', c('NPP, RESP'), col=c('darkgreen','red'), lwd=c(6,6))
grid(col='black')
# Bottom panel is carbon pools
plot.limits <- c(0,max(c(plant, litter, fast.soil, slow.soil),na.rm=TRUE))
plot.col = c('darkgreen','darkolivegreen2','burlywood','chocolate4')
plot(years, plant, main='Carbon Pools', ylab='GtC', col=plot.col[1],
ylim=plot.limits)
lines(years, litter, col=plot.col[2], type='l', lwd=6)
lines(years, fast.soil, col=plot.col[3], type='l', lwd=6)
lines(years, slow.soil, col=plot.col[4], type='l', lwd=6)
legend('topright', c('Plants','Litter','Fast','Slow'), col=plot.col,
lwd=rep(6,4))
grid(col='black')
detach(history)
par(orig.par)
}
plot.input <- function(history){
attach(history)
years <- 1800:2299
orig.par <- par(no.readonly=TRUE)
par(mfrow=c(4,1), cex.lab=1.5, cex.axis=1.5, cex.main=2)
# Top panel is CO2
plot(years, CO2, main='Atmospheric CO2', ylab='ppmv',
type='l', lwd=6)
grid(col='black')
# Second panel is nutrient status
plot(years, nutrient, main='Nutrient Status', ylab='ratio', col='darkgreen',
type='l', lwd=6)
grid(col='black')
# Third panel is deforestation
plot(years, disturbance, main='Deforestation', ylab='GtC/yr', col='red',
type='l', lwd=6)
grid(col='black')
# Fourth panel is temperature
plot(years, temp, main='Warming', ylab='Celsius', col='red',
type='l', lwd=6)
grid(col='black')
detach(history)
par(orig.par)
}
library(shiny)
install.packages('shiny')
install.packages('markdown')
library(shiny)
library(markdown)
responses<- read.table("C:/Temp/Dropbox/Phd/Energy3D/Fifth/Independent Studies/Pilot Data/AnovaAnalysisv3.csv",header=TRUE,sep=",")
fit<-with(responses, aov(SquaredSys~RespnseType))
summary(fit)
TukeyHSD(fit)
fit<-with(responses, aov(SquaredExp~RespnseType))
summary(fit)
TukeyHSD(fit)
# Package to generate latex code
#install.packages('xtable')
library(xtable)
# Packages for plotting and having multiple plots in a single screen
#install.packages('ggplot2')
library(ggplot2)
#install.packages('gridExtra')
library(gridExtra)
#install.packages('Hmisc')
library(Hmisc)
# Package for Inferential Statistics
library(coin)
#install.packages('exactRankTests')
library(exactRankTests)
# Set the working directory
setwd("C:/Temp/Dropbox/Phd/Administrative/Mimi/Scripts/")
# Read the data from the csv file
data <- read.table("SurveyData.csv",header=TRUE,sep=",")
summary(data)
# Subset the data between graduate and undergrads
gradData <- data[data$Level=='G',1:6]
underData <- data[data$Level=='U',c(1,7:13)]
library(reshape)
library(plyr)
library(Rmisc)
# Graduate Plots
plotPreferences <-  function(lessData, greatedData, act)
{
result<- rbind(lessData,greatedData)
colnames(result) <- c('Student.ID', act,'group')
meltedResult <- melt(result, id.vars=c("Student.ID", "group"))
# Compute means and standard error (se)
means <- ddply(meltedResult, c("group", "variable"), summarise,
mean=mean(value, na.rm = TRUE), se=sd(value, na.rm = TRUE)/sqrt(length(value)))
# Create the plot with the error bars
ggplot(means, aes(x=variable, y=mean, fill= group)) +
geom_bar(position=position_dodge(), stat="identity")+
scale_colour_grey() +
labs(x= 'Learning Activity', y='Average Preference per Group')+
geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2, position=position_dodge(.9)) +
scale_fill_grey(guide = guide_legend(title = "Group"), start = 0.2, end = .5)+
scale_y_continuous(breaks=c(-3:10))
}
# Scatter plots among questions
data2 <- aggregate(gradData$G1,by=list(x=gradData$G1,y=gradData$G2),length)
names(data2)[3] <- "count"
a<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q1 VS Q2") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G1,by=list(x=gradData$G1,y=gradData$G3),length)
names(data2)[3] <- "count"
b<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q1 VS Q3") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G1,by=list(x=gradData$G1,y=gradData$G4),length)
names(data2)[3] <- "count"
c<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q1 VS Q4") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G1,by=list(x=gradData$G1,y=gradData$G5),length)
names(data2)[3] <- "count"
d<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q1 VS Q5") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G2,by=list(x=gradData$G2,y=gradData$G3),length)
names(data2)[3] <- "count"
e<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q2 VS Q3") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G2,by=list(x=gradData$G2,y=gradData$G4),length)
names(data2)[3] <- "count"
f<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q2 VS Q4") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G2,by=list(x=gradData$G2,y=gradData$G5),length)
names(data2)[3] <- "count"
g<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q2 VS Q5") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G3,by=list(x=gradData$G3,y=gradData$G4),length)
names(data2)[3] <- "count"
h<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q3 VS Q4") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G3,by=list(x=gradData$G3,y=gradData$G5),length)
names(data2)[3] <- "count"
i<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q3 VS Q5") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
data2 <- aggregate(gradData$G4,by=list(x=gradData$G4,y=gradData$G5),length)
names(data2)[3] <- "count"
j<- ggplot(data2, aes(x = x, y = y)) +  ggtitle("Q4 VS Q5") + geom_abline(slope = 1) + geom_point(aes(size = count)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10))
multiplot(a,b,c, cols=3)
